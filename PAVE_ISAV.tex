\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{minted}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{PAVE: An In Situ Framework for Scientific Visualization and Machine Learning Coupling\\
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Samuel Leventhal}
\IEEEauthorblockA{\textit{University of Utah School of Computing} \\
\textit{Scientific Computing and Imaging Institute}\\
Salt Lake City, UT., USA \\
samlev@cs.utah.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Mark Kim}
\IEEEauthorblockA{\textit{Oak Ridge National Laboratory} \\
Oak Ridge TN., USA \\
kimmb@ornl.gov}
\and
\IEEEauthorblockN{3\textsuperscript{rd} David Pugmire}
\IEEEauthorblockA{\textit{Oak Ridge National Laboratory} \\
Oak Ridge TN., USA \\
pugmire@ornl.gov}
}

\maketitle

\begin{abstract}
    The need for researchers and practitioners to have access to {\it in situ} operations is increasing for artificial intelligence and visualisations tasks as implementations become more complex and scaled widening the gap between computation time and IO throughput. What is more, a growing number of applications continue to be developed dealing with the combination of these two domains. With PAVE we present a solution which allows in place data management between visualisation and machine learning tasks. We then demonstrate our framework with the application of a conditional Generative Adversarial neural Network (cGAN) capable of in situ and at scale training over path traced images resulting in a generative model able to produce real-time scene renderings with accurate light transport and global illumination of a quality comparable to offline approaches. 
    
\end{abstract}

\begin{IEEEkeywords}
    VTKm, neural networks, generative adversarial network, in-situ, PyTorch, path tracing
\end{IEEEkeywords}


% \begin{teaserfigure}
%     \includegraphics[width=\textwidth]{buffer_results_teaser.png}
%     \caption{\textmd{Rendered Conditional Geometry Buffers ({\bf left set}) and artificial rendering with conditional generative adversarial neural network ({\bf right couple}) comparing ground truth path traced rendering ({\bf left}) with image generated ({\bf right}).}}
%     \Description{Conditional Buffers and path traced rendered with VTKm to be used for training a PyTorch conditional generative adversarial network.}
%     \label{teaser}
%   \end{teaserfigure}
\input{Intro.tex}

\input{RelatedWorks.tex}
\input{PAVE.tex}
\input{CaseStudy1.tex}


\subsection{Our Provided Example of PAVE}\label{ex}

For our PyTorch in situ proposal the current systems we employ are VTK-m, for rendering path traced images and conditional geometry buffers focus, and Adios2 for data management. We discuss the design pattern for our in situ visualisation task with support from deep learning in the order of operations followed within the pipeline of use. Namely, we present the design pattern for rendering light transport in VTK-m coupled with data transport to PyTorch (\ref{pathtracer}). Subsequently we explain the infrastructure for embedding VTK-m throughput managed by Adios2 within PyTorch and demonstrate through example (\ref{pytorch}) by  instantiating a PyTorch data interface allowing for data parallelization and multi-GPU/distributed training as used within the framework for training our cGAN model.

\subsubsection{VTK-m Light Transport Visualisation}\label{pathtracer}

Path traced images are maintained within C++11 as VTK-m arrays which can be passed by reference directly to PyTorch using Adios2 APIs or written to Adios2 .bp file and retrieved during training or when utilising the neural network to generate novel scene renderings from rendered geometry buffers. 

\subsubsection{PyTorch cGAN Global Illumination Generator}\label{pytorch}

For training, our solution used by the cGAN is ``{\it AdiosDataLoader}'', a data class inheriting from the abstract indexing class PyTorch {\it torch.utils.data.Dataset}. The {\it AdiosDataLoader} employs Adios2 to either retrieve from file or have passed by reference vector representations of path traced images and conditional buffers. Within VTK-m during generation these vectors represented as VTK-m vectors and within PyTorch as numpy arrays. In this manner the training or test sets needed by PyTorch and created by VTK-m are available to PyTorch in situ or with reference to written memory. If retrieving VTK-m's renderings PyTorch will compile Adios2 attributes from file as tabled by Adios2 into .bp files. VTK-m generated datasets can be retrieved with {\it read\_adios\_bp()} or passed to a similar {\it get\_adios\_bp()} and subsequently forwarded to our {\it get\_split()}  to partition the dataset into 60\% training, 20\% testing and 20\% validation subsets. The split datasets are then used to construct the {\it AdiosDataLoader} class which inherits from the {\it torch.utils.data.DataLoader} thereby providing a data sampler of our VTK-m renderings with a single-process or multi-process iterator over the dataset affording the tools necessary to train our neural networks in the canonical manner.

% other example scripts
%\inputminted{python}{pytorchAdiosRead.py}
%\inputminted{python}{trainingSplit.py}




In the above code sample the {\it AdiosDataLoader} class is used to partition the data set into training, validation and testing as well as offer a distributable data sampler with an array-like data structure allowing index access to elements and collection size functionality.

%\noindent\rule{0.5\textwidth}{1pt}
%\inputminted{python}{adiosdataloader.py}
%\noindent\rule{0.5\textwidth}{1pt}

\section{Cornell Box Experiment}

To evaluate the quality of in situ deep learning aided visualisations train the cGAN networks on rendered images of a Cornell box, a commonly used 3D modelling framework for quality assessment. We train the model using renderings of the Cornell box with high light sample count and depth computation per ray for various camera angle perspectives into the box along with the associated image geometry buffers for a given camera orientation. We then assess the quality of the models final generated renderings looking at the accuracy of global illumination. We then also demonstrate the performance of the models ability to render global illumination when given image buffers for a novel scene not used for training similar in content but not exact. The scene used for training is comprised of the classic set up with one overhead light source in the center of a white ceiling, a white back wall and a white floor. The remaining walls are then colored red on the left and green on the right in order to afford different colored light transport and demonstrate diffuse interreflection. The contents of the Cornell box are three cuboids of various shapes and sizes to provide diverse shading and diffused lighting. 

%\vspace{-1.5em}
\begin{figure}[h]
\includegraphics[width=0.25\textwidth]{sc-1080-d-45.png}
\end{figure}
%\vspace{-1em}

The conditional differed shading geometry buffers used are direct lighting, normal planes, depth and albedo as shown in figure \ref{teaser}.
\iffalse
\begin{figure}[h]
\caption{\textmd{Global illumination conditional image buffers. {\bf Top:} Albedo, left. Depth, right. {\bf Bottom:} Normals, left. Direct lighting, right.}}
\includegraphics[width=0.40\textwidth]{demo.png}
\label{Gbuf}
\end{figure}
\fi
The geometry buffers serve as joint variables for the conditional probability distribution which the global illumination path traced images are considered to exist. The conditional arguments in this experiment then aid the cGAN in learning behavior of light paths given the geometry of a scene in question. 

\input{Results.tex}
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\section*{Acknowledgment}
 This research was supported in part by an appointment to the Oak Ridge National Laboratory ASTRO Program, sponsored by the U.S. Department of Energy and administered by the Oak Ridge Institute for Science and Education.



\bibliographystyle{IEEEtran}
\bibliography{pave_ref}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}

\section{Related Work}
\subsection{In situ Data Management}
As output sizes of simulations have grown to petabytes of data, new strategies are required to handle the volumes of data generated. In situ analysis and visualization frameworks have emerged to handle this deluge of data, dependent on the needs and requirements of the HPC application~\cite{Abbasi2010, Ayachit:2015:PCE:2828612.2828624,Childs:VisIt-HPV-Chapter:2012, 6846460}. Some frameworks provide a generic data interface~\cite{Ayachit:2016:SGS:3018859.3018867,Larsen:2017:ASI:3144769.3144778} with triggers~\cite{Larsen:2018:FSS:3281464.3281468}. Others focus on in situ, IO-oriented approaches~\cite{doi:10.1002/cpe.3125}, which enables new paradigms for in situ performance~\cite{Kress:ISC19}.

\subsection{Heterogeneous Visualization}
Heterogeneous environments in HPC require new solutions for extracting the most performance. The accelerator, such as GPGPU, in these heterogeneous environments have thousands of concurrent threads with various APIs, such as CUDA~\cite{CUDA} or Thread Building Blocks~\cite{books:daglib:0018624}. Thrust~\cite{hoberock2009thrust} is a proprietary API for parallel primitives programming~\cite{Blelloch:1990:VMD:91254} on Nvidia hardware. VTK-m is a data parallel primitive toolkit for scientific visualization~\cite{vtkm} which has a general parallel programming model with multiple backends to facilitate porting to different heterogeneous systems. 

\subsection{Path Tracing}
Real time, true to life quality renderings of light transport remains an active area of research with a number of various approaches. To preserve real-time rates, previous works have stored precomputed radiance transfers for light transport as spherical functions within a fixed scene geometry which are then adjusted for varied light and camera perspective through projections within a basis of spherical harmonics \cite{sloanPrecompRad}. Similarly, Light Propagation Volumes have been used to iteratively propagate light between consecutive grid positions to emulate single-bounce indirect illumination \cite{kaplanyanCasac}. More recently, deep neural  networks have been employed as a learned look up table for real-time rates with offline quality. With the use of convolutional neural networks Deep Shading is able to translate screen space buffers to into desired screen space effects such as indirect light, depth or motion blur. Similar to the methodology implemented in this work, Deep Illumination uses a conditional adversarial network (cGAN) to train a generative network with screen space buffers allowing for a trained network able to produce accurate global illumination with real-time rates at offline quality through a ``one network for one scene'' setting  \cite{deepillum}.